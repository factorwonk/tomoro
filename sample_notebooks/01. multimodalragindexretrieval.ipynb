{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', -1)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Helper Ingestion Pipeline Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_string(table):\n",
    "    \"\"\"Convert table to string representation.\"\"\"\n",
    "    return \"\\n\".join(\" | \".join(str(cell) for cell in row) for row in table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_summarization_chain():\n",
    "    \"\"\"Set up the summarization chain for text and table content.\"\"\"\n",
    "    prompt_text = \"\"\"\n",
    "    You are an assistant tasked with summarizing tables and text from financial documents for semantic retrieval.\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements.\n",
    "    Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "    For any tables also add in a description of what the table is about besides the summary.\n",
    "    Then, include the table in markdown format. Do not add additional words like Summary: etc.\n",
    "\n",
    "    Table or text chunk:\n",
    "    {element}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    chatgpt = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    return (\n",
    "        {\"element\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | chatgpt\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text: str, chunk_size: int = 4000, chunk_overlap: int = 500) -> List[str]:\n",
    "    \"\"\"Create chunks from text using RecursiveCharacterTextSplitter.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(json_data: List[Dict]) -> tuple:\n",
    "    \"\"\"Process JSON data and return text and table documents with their summaries.\n",
    "    Only processes entries with valid qa fields (not None or empty dict).\"\"\"\n",
    "    processed_data = []\n",
    "    for entry in json_data:\n",
    "        # Check if qa field exists and is not empty\n",
    "        qa_field = entry.get(\"qa\")\n",
    "        if qa_field is None or qa_field == {}:\n",
    "            continue\n",
    "        \n",
    "        text_parts = []\n",
    "        if entry.get(\"pre_text\"):\n",
    "            text_parts.append(\" \".join(entry[\"pre_text\"]))\n",
    "        if entry.get(\"post_text\"):\n",
    "            text_parts.append(\" \".join(entry[\"post_text\"]))\n",
    "        \n",
    "        table = entry.get(\"table_ori\", \"\")\n",
    "        table_str = table_to_string(table) if table else \"\"\n",
    "            \n",
    "        full_text = \"\\n\\n\".join(text_parts)\n",
    "            \n",
    "        processed_data.append({\n",
    "            \"id\": entry.get(\"id\"),\n",
    "            \"text\": full_text,\n",
    "            \"table\": table_str,\n",
    "            #\"table_ori\": table_ori,\n",
    "            \"qa\": entry.get(\"qa\", {})\n",
    "        })\n",
    "    \n",
    "    text_docs = []\n",
    "    table_docs = []\n",
    "    \n",
    "    summarize_chain = setup_summarization_chain()\n",
    "    \n",
    "    for item in processed_data:\n",
    "        if item[\"text\"]:\n",
    "            chunks = create_chunks(item[\"text\"])\n",
    "            for chunk in chunks:\n",
    "                text_docs.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"parent_id\": item[\"id\"]}\n",
    "                ))\n",
    "        \n",
    "        if item[\"table\"]:\n",
    "            table_docs.append(Document(\n",
    "                page_content=str(item[\"table\"]),\n",
    "                metadata={\"parent_id\": item[\"id\"], \"is_table\": True}\n",
    "            ))\n",
    "    \n",
    "    text_summaries = summarize_chain.batch([doc.page_content for doc in text_docs], {\"max_concurrency\": 5})\n",
    "    table_summaries = summarize_chain.batch([doc.page_content for doc in table_docs], {\"max_concurrency\": 5})\n",
    "    \n",
    "    return text_docs, text_summaries, table_docs, table_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_content_types(docs):\n",
    "    \"\"\"Split retrieved documents into text and table content.\"\"\"\n",
    "    texts = []\n",
    "    tables = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        if doc.metadata.get(\"is_table\", False):\n",
    "            tables.append(doc.page_content)\n",
    "        else:\n",
    "            texts.append(doc.page_content)\n",
    "    \n",
    "    return {\"texts\": texts, \"tables\": tables}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Helper Retrieval-Generation Pipelne Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables):\n",
    "    \"\"\"Create retriever that indexes summaries but returns raw content.\"\"\"\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "    \n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "    \n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "    \n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_function(data_dict):\n",
    "    \"\"\"Create a prompt with text and table context.\"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    formatted_tables = \"\\n\".join(data_dict[\"context\"][\"tables\"])\n",
    "    \n",
    "    prompt_text = f\"\"\"You are an analyst tasked with understanding detailed information from text documents and data tables.\n",
    "        Use the provided context information to answer the user's question.\n",
    "        Do not make up answers, use only the provided context documents below.\n",
    "\n",
    "        User question:\n",
    "        {data_dict['question']}\n",
    "\n",
    "        Text context:\n",
    "        {formatted_texts}\n",
    "\n",
    "        Table context:\n",
    "        {formatted_tables}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    return [HumanMessage(content=prompt_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_chain(retriever, chatgpt):\n",
    "    \"\"\"Set up the RAG chain.\"\"\"\n",
    "    # Create base RAG chain\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter('context'),\n",
    "            \"question\": itemgetter('input'),\n",
    "        }\n",
    "        | RunnableLambda(prompt_function)\n",
    "        | chatgpt\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Create retrieval chain\n",
    "    retrieve_docs = (\n",
    "        itemgetter('input')\n",
    "        | retriever\n",
    "        | RunnableLambda(split_content_types)\n",
    "    )\n",
    "\n",
    "    # Combine into final chain\n",
    "    return (\n",
    "        RunnablePassthrough.assign(context=retrieve_docs)\n",
    "        .assign(answer=rag_chain)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_qa(chain, query):\n",
    "    \"\"\"Execute RAG QA.\"\"\"\n",
    "    response = chain.invoke({'input': query})\n",
    "    print('=='*50)\n",
    "    print('Answer:')\n",
    "    display(Markdown(response['answer']))\n",
    "    print('--'*50)\n",
    "    print('Sources:')\n",
    "    text_sources = response['context']['texts']\n",
    "    table_sources = response['context']['tables']\n",
    "    \n",
    "    if text_sources:\n",
    "        print(\"\\nText Sources:\")\n",
    "        for text in text_sources:\n",
    "            display(Markdown(text))\n",
    "            print()\n",
    "    \n",
    "    if table_sources:\n",
    "        print(\"\\nTable Sources:\")\n",
    "        for table in table_sources:\n",
    "            display(Markdown(f\"```\\n{table}\\n```\"))\n",
    "            print()\n",
    "    \n",
    "    print('=='*50)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Execute Main Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize models\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    chatgpt = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    # Initialize Chroma vectorstore\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"text_table_rag\",\n",
    "        embedding_function=embedding_model,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    \n",
    "    # Load and process data\n",
    "    with open('./data/convfinqatrain.json', 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    test_data = json_data[:10]\n",
    "    \n",
    "    # Process documents and generate summaries\n",
    "    # text_docs, text_summaries, table_docs, table_summaries = process_documents(test_data)\n",
    "    \n",
    "    # Process documents with progress bar\n",
    "    print(\"Processing documents and generating summaries...\")\n",
    "    with tqdm(total=1, desc=\"Processing Documents\") as pbar:\n",
    "        text_docs, text_summaries, table_docs, table_summaries = process_documents(test_data)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"Processed {len(text_docs)} text documents and {len(table_docs)} table documents\")\n",
    "\n",
    "    # Create retriever\n",
    "    print(\"\\nEmbedding into doctstore and vectorstore...\")\n",
    "    retriever = create_multi_vector_retriever(\n",
    "        vectorstore,\n",
    "        text_summaries,\n",
    "        text_docs,\n",
    "        table_summaries,\n",
    "        table_docs\n",
    "    )\n",
    "    print(\"Embedding complete!\")\n",
    "\n",
    "    # Setup RAG chain\n",
    "    rag_chain = setup_rag_chain(retriever, chatgpt)\n",
    "    \n",
    "    return rag_chain, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rag_chain, retriever = main()\n",
    "    \n",
    "    # Example usage\n",
    "    question = 'what was the percentage change in the net cash from operating activities from 2008 to 2009?'\n",
    "    \n",
    "    # Test retriever directly\n",
    "    print(\"\\nTesting retriever directly:\")\n",
    "    docs = retriever.invoke(question, limit=5)\n",
    "    print(f\"Retrieved {len(docs)} documents\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Content preview: {doc.page_content[:200]}...\")\n",
    "    \n",
    "    # Get full response\n",
    "    print(\"\\nGetting full response:\")\n",
    "    response = rag_qa(rag_chain, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmengg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
